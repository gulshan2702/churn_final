# -*- coding: utf-8 -*-
"""customer_Churn_prediction_using_ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/114T0FexoqqbqdxPn9f-FqSy7F0ly6CMY
"""

from google.colab import drive
drive.mount('/content/gdrive/')

# Importing useful Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Importing the datasets

dataset = pd.read_csv("/content/gdrive/My Drive/customer_churn_prediction/Churn_Modelling.csv")

dataset.info()
dataset.shape

dataset.head()

# Since RowNumber,CustomerId And Surname doesn't provide much information on predicting the customer churning behaviour in a bank
# So, we remove those columns in our dataset

X = dataset.iloc[:,3:13]
y = dataset.iloc[:,13]

X.shape

# Printing the  feature values of our dataset

X

# Printing the labels of our datasets

y

# By observing the feature values ,we know that country and gender are categorical values in the dataset and 
#  while building our machine learning models the categorical varaibale and values are not allowed. So we need to encode those categorical data

# Encoding Categorical Data

from sklearn.preprocessing import LabelEncoder,OneHotEncoder

label_X_1 = LabelEncoder()
X['Geography'] = label_X_1.fit_transform(X['Geography'])
label_X_2 = LabelEncoder()
X['Gender'] = label_X_2.fit_transform(X['Gender'])

# 0 stans for France , 2 stands for Spain and 1 stands for germany
# 0 stands for Female and 1 stands for Male

X

# Since we are encoding three different countries France , Spain and germany as 0 , 2 and 1 . However there are not any relationship between these countries
# but encoding them like this shows that Spain is greater than germany and France mathematically. So for this purpose we need to perform one hot encoding.
X.Geography.values.shape

onehotencoder = OneHotEncoder()
ohe = onehotencoder.fit_transform(X.Geography.values.reshape(-1,1)).toarray()

# 0 stans for France , 2 stands for Spain and 1 stands for germany
# 0 stands for Female and 1 stands for Male

X.head(5)

i = 0
for items in ohe:
  print(items)
  i+= 1
  if(i==5):
    break

encoded_df = pd.DataFrame(ohe,columns=['France','Germany','Spain'])

encoded_df

import pandas as pd

X = pd.concat([encoded_df,X],axis=1)

X

# Removing one dummy feature / variable / columns.
# Dropping Geography columns and one dummy variable columns i.e. France 
#

preprocessed_dataframe = X.drop(['France','Geography'],axis=1)

preprocessed_dataframe.head()

trainable_data = preprocessed_dataframe.iloc[:,:].values

trainable_data[0]

trainable_labels = dataset.iloc[:,13].values
trainable_labels

trainable_data.shape

# Splitting the dataset into Training set and Test set

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(trainable_data,trainable_labels,test_size=0.2,random_state=0)

#Feature Scaling

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

# Now Data preprocessing step is finished now we must focus on building the architecture of ANN

#Importing the Keras Libraries and Packages

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Activation
import tensorflow as tf

classifier = Sequential()

#Adding the input layer and the first hidden layer

classifier.add(Dense(units=8,kernel_initializer='uniform',input_dim=11))
classifier.add(Activation('relu'))

#Adding the second hidden layer

classifier.add(Dense(units = 8,kernel_initializer='uniform',activation='relu'))

#Adding the output layer

classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))

#Compiling the ANN model

classifier.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False),loss='binary_crossentropy',metrics=['accuracy'])

# Fitting the ANN to the trainable splits

history = classifier.fit(X_train,y_train,batch_size=25,epochs=100)

classifier.evaluate(X_test,y_test,verbose=2)

from sklearn.metrics import confusion_matrix
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

cn = confusion_matrix(y_test,y_pred)

cn

acc = history.history['accuracy']
# val_acc = history.history['val_accuracy']

loss = history.history['loss']
# val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
# plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
# plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training  Accuracy')

# plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
# plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and  Loss')
plt.xlabel('epoch')
plt.show()

# Cross validating the model using splitting validation datasets by 20%

train_valid_history =classifier.fit(X_train,y_train,batch_size=25,epochs=100,validation_split=0.2)

acc = train_valid_history.history['accuracy']
val_acc = train_valid_history.history['val_accuracy']

loss = train_valid_history.history['loss']
val_loss = train_valid_history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

#Time for prediction
# Predict a single new observation if the customer with the following information will leave the bank or not

# Geography: France
# Credit Score : 600
# Gender : Male
# Age : 40
# Tenure : 3
# Balance : 60000
# No.of products: 2
# Has credit card : Yes
#  Is Active Member : Yes
#  Estimated Salary : 50000

preprocessed_dataframe

new_prediction_score = classifier.predict(sc.transform(np.array([[0,0,600,1,40,3,60000,2,1,1,50000]])))

new_prediction = (new_prediction_score > 0.5)
if(new_prediction==False):
  print("The customer won't leave the bank")
else:
  print("The customer will leave the bank")

# K-Fold Cross Validation Strategy for getting the accurate model accuracy

import sklearn
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score

# Building a function to load the keras model

def build_model():
  model = Sequential()
  model.add(Dense(units=8,kernel_initializer='uniform',input_dim=11))
  model.add(Activation('relu'))
  model.add(Dense(units = 8,kernel_initializer='uniform',activation='relu'))
  model.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False),loss='binary_crossentropy',metrics=['accuracy'])
  return model

model = KerasClassifier(build_fn=build_model,epochs=100,batch_size=25)

# Performing cross validation technique in all of our datasets

trainable_data.shape

len(trainable_labels)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
k_fold_train_valid = sc.fit_transform(trainable_data)

k_fold_train_valid.shape

cross_val_accuraies = cross_val_score(estimator=model,X=k_fold_train_valid,y=trainable_labels,cv=5,n_jobs=-1)

mean = cross_val_accuraies.mean()
variance = cross_val_accuraies.std()

print("The mean accuracy of our model after using k-fold cross validation is : ",mean)
print("The minimum accuracy of our model after using k-fold cross validation is : ",min(cross_val_accuraies))
print("The maximum accuracy of our model after using k-fold cross validation is : ",max(cross_val_accuraies))

# Tuning the hyperparameters of ANN

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

from keras.models import Sequential
from keras.layers import Dense
import tensorflow as tf

def build_classifier(units):
  tuning_model = Sequential()
  tuning_model.add(Dense(units=units,kernel_initializer='glorot_uniform',activation='relu',input_dim=11))
  tuning_model.add(Dense(units=units,kernel_initializer='glorot_uniform',activation='relu'))
  tuning_model.add(Dense(units=1,kernel_initializer='glorot_uniform',activation='sigmoid'))
  tuning_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),loss='binary_crossentropy',metrics=['accuracy'])
  return tuning_model

tuning_model = KerasClassifier(build_classifier)

#  Choosing the hyperparameters by our ANN automatically for optimizing the accuracy of our models

parameters = {'batch_size':[16,32],
              'epochs':[150,200],
              'units':[16,32]}

grid_search = GridSearchCV(estimator = tuning_model,param_grid=parameters,scoring='accuracy',cv=5)

grid_search = grid_search.fit(X=X_train,y=y_train)

best_parameters = grid_search.best_params_

best_accuracy = grid_search.best_score_

print(best_parameters)

print(best_accuracy)

# Applying best parameters and checking the model accuracy

best_model = Sequential()
best_model.add(Dense(units=16,kernel_initializer='uniform',input_dim=11))
best_model.add(Activation('relu'))
best_model.add(Dense(units = 16,kernel_initializer='uniform',activation='relu'))
best_model.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))
best_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False),loss='binary_crossentropy',metrics=['accuracy'])

best_model.fit(x=X_train,y=y_train,batch_size=16,epochs=200,validation_split=0.2)

best_model.fit(X_test,y_test,verbose=2)

# Since,batch_size of 25 is not included in our parameter tuning but 25 is the perfect batch size for getting good performance of our models. So checking
# the accuracy with other hyperparameters obtained from parameter tuning with batch_size of 25

best_model.fit(x=X_train,y=y_train,batch_size=25,epochs=200,validation_split=0.2)

best_model.evaluate(X_test,y_test)

best_model = Sequential()
best_model.add(Dense(units=16,kernel_initializer='uniform',activation='relu',input_dim=11))
best_model.add(Dense(units = 16,kernel_initializer='uniform',activation='relu'))
best_model.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))
best_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

best_model.fit(x=X_train,y=y_train,batch_size=25,epochs=200,validation_data=(X_test,y_test))

rms_model = Sequential()
rms_model.add(Dense(units=16,kernel_initializer='uniform',activation='relu',input_dim=11))
rms_model.add(Dense(units = 16,kernel_initializer='uniform',activation='relu'))
rms_model.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))
rms_model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])

final_history = rms_model.fit(x=X_train,y=y_train,batch_size=25,epochs=200,validation_data=(X_test,y_test))

# From all the computations we can say that rms prop optimizer perform better than adam optimizer. So we the final model has 16 hidden layers with batch 
# size of 25 and optimizer equals adam and the epochs is equal to 200 and the average validation and testing accuracy is 86%

acc = final_history.history['accuracy']
val_acc = final_history.history['val_accuracy']

loss = final_history.history['loss']
val_loss = final_history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

# Saving our model and it's architecture
# Since rms prop is performing well so we serailize rms_prop model to use it further for deployment.

# serialize model to JSON
model_json = rms_model.to_json()
with open("customer_churn_prediction_model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
rms_model.save_weights("customer_churn_prediction_model.h5")
print("Saved model to disk")

# Saving the model weights in your google drive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)  

# get the folder id where you want to save your file
file = drive.CreateFile({'parents':[{u'id': '1WPJKqUwcgnQz6aMP2yZM9l4d-71cUyBa'}]})
file.SetContentFile('/content/customer_churn_prediction_model.h5')
file.Upload()

# Saving your model architecture in your google drive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)  

# get the folder id where you want to save your file
file = drive.CreateFile({'parents':[{u'id': '1WPJKqUwcgnQz6aMP2yZM9l4d-71cUyBa'}]})
file.SetContentFile('/content/customer_churn_prediction_model.json')
file.Upload()

